Power Series Example and Benchmarking Lazy Numeric Computation

Hi,

I was recently made aware of some of the things you guys are up to over here on the Numeric group when I proposed a patch on the Dev list related to a small symbolic algebra program I had been working on and Mike just so happened to suggest I check out Expresso and also using core.matrix for speeding up scalar math. I guess it just goes to show that sometimes value of seeing through hard problems is you'll be keyed into better solutions from people who've been working on them even longer.

My interests currently lie in problems that can be solved outside the domain of numerical analysis, such as symbolic computation and lazy evaluation. With that in mind, I'll start by sharing an old favorite everyone has probably seen: a tiny library for lazy computation of power series https://github.com/Sophia-Gold/power-series.clj. 

What inspired me to whip this up in Clojure was being introduced to transducers while playing with core.async. At first, I was impressed just by the flexibility of being able to apply an algorithm to any input and output (and two-thirds of the functions includes in this library are just variations on map, while the rest build on top of those), but then when I actually wrote my own transducer for the first time I ran some naive speed tests on it and couldn't believe what I was seeing. It was doing all the algorithmic heavy lifting, but pretty much anything I could apply it to would slow it down by a polynomial factor. 

I bring this up not just to wax theoretical or bask in awe, but for two reasons:
There seems to be a perception that Clojure is a somehow good for processing large amounts of data, but bad for numerical computing. I've heard this even from people inside the community. On the one hand, as I've learned from Mike, this can be fixed by instituting best practices for time-sensitive math. But I also think a lot of the reason why we're seeing dynamic languages be considered for numeric computing at all recently is because there's an increasing relationship between performance and fungibility of parameters in the problem domain, which is exactly what such abstract combinators allow for.
In Clojure this flexibility applies implicitly to lazy evaluation, which takes numerical computing in an entirely different direction than the work being done with core.matrix and parallelism in general. This seems underexploited when it comes to numerics, likely due to opinions on the viability of laziness for efficient problem solving entirely. With that in mind, I decided to run some slightly more concrete benchmarks on some examples from the power series code using Criterium and include them in the repo. I couldn't find much background on benchmarking lazy computation in general so would appreciate any feedback on them.
A cursory search of the list found this blog post benchmarking vectorz-clj vs. protocols, which provided quite a helpful point of  comparison: https://clojurefun.wordpress.com/2013/03/07/achieving-awesome-numerical-performance-in-clojure/. Using lazy evaluation, I was able to generate a cosine series (defined recursively) to the point of convergence at almost exactly the same speed as adding two core.matrix vectors using mutation (comparing an operation using division to one using addition). 

On the far other of complexity, I was able to generate a tangent series to the point of convergence by dividing two other infinitely convergent series through calculating the compositional inverse of their Cauchy products, or the convolution between terms in roughly one-third of the time it took to add two clojure vectors. This is the most expensive operation in the library and surely could be accelerated further by both sacrificing the integrity of the divisor and dividend and, more importantly, giving type hints so as to avoid conversion of ratios to BigIntegers (the latter applying to nearly all series produced by the library).

Infinite series are somewhat paradoxical benchmarks because, although they grow linearly with number, the rate is exponential with regards to storage due to the expansion of each term. However, because these particular series are convergent what would be expected to be exponential turns out to be constant rather quickly: in the case of cosine and the exponential function only roughly three times the overhead. In that sense, it's quite a different metric for lazy evaluation than the Sieve of Eratosthenes, where computation takes longer the further it progresses. Although somewhat skeptical of the value of benchmarking, I would be interested to see comparisons between languages for both types of lazy numeric computation: particularly with the Haskell versions.

In general, I'd like to see more comparison of benchmarks in common numeric applications between Clojure and other languages, especially outside of the JVM. It seems like we're at a great time as far as library and community support to position it as a general option for a dynamic language, among others, to use for HPC.

Lastly, and I know brevity isn't my strong suit, I seem to always leave even the tiniest project with a new weird idea and wanted to float it here. I had the idea for a lazy version of both BigInteger and BigDecimal types that would allow essentially limitless precision at no storage cost, with of course the oddity that one would be streaming a portion of the large number. I'm wondering if anyone can think of use cases for this? Applications involving lossless compression, such as arithmetic coding come to mind, but other than having implemented versions as exercises that's far from my area of expertise. It seems almost trivial to implement, so I'll likely end up doing it and come back to share...however odd the idea :)

Thanks,
Sophia
