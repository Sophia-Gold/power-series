Power Series Example and Benchmarking Lazy Numeric Computation

Hi,

I was recently made aware of some of the things you guys are up to over here in the Numerics group when I proposed a patch on the Dev list related to a small symbolic algebra program I had been working on and Mike just so happened to suggest I check out Expresso and also using core.matrix for speeding up scalar math. I was very impressed and am hoping to get involved in some form or another.

Although I have some background in hardware APIs from doing graphics work, my interests currently lie in problems that can be solved outside the domain of numerical analysis, such as symbolic computation and lazy evaluation. With that in mind, I'll start by sharing an old favorite everyone has probably seen before: a tiny library for lazy computation of power series https://github.com/Sophia-Gold/power-series.clj. 

What inspired me to whip this up in Clojure was being introduced to transducers while playing with core.async. At first, I was impressed just by the flexibility of being able to apply an algorithm to any input and output (two-thirds of the functions in this library are just variations on map, while the rest build on top of those), but then when I actually wrote my own transducer for the first time I ran some naive speed tests on it and couldn't believe what I was seeing. It was doing all the algorithmic heavy lifting, but pretty much anything I applied it to would slow it down by a polynomial factor. 

I bring this up not just to wax theoretical or bask in awe, but for two reasons:
There seems to be a perception that Clojure is a somehow good for processing large amounts of data, but too slow for numerical computing. I've heard this even from people inside the community. On the one hand, as I've learned from Mike, this can be fixed by instituting best practices for time-sensitive math. But I also think a lot of the reason why we're seeing dynamic languages be considered for numeric computing at all recently is because there's an increasing relationship between performance and fungibility of parameters in the problem domain, which is exactly what such abstract combinators allow for.
In Clojure this flexibility is implicitly lazy, which takes numerical computing in an entirely different direction than the work being done with core.matrix and parallelism in general. This seems underexploited when it comes to numerics, likely due to entrenched opinions on the viability of laziness for efficient problem solving. With that in mind, I decided to run some slightly more concrete benchmarks on examples from the power series code using Criterium and include them in the repo. I couldn't find much background on benchmarking lazy computation in general so would appreciate any feedback on them.
A cursory search of the list found this blog post benchmarking vectorz-clj vs. protocols, which provided a very helpful point of comparison: https://clojurefun.wordpress.com/2013/03/07/achieving-awesome-numerical-performance-in-clojure/. 

Using lazy evaluation, I was able to generate a cosine series (defined recursively) to the point of convergence at almost exactly the same speed as adding two core.matrix vectors using mutation (the former using core.math division on an exponentially greater number of elements). 

At the far other end of complexity, I was able to generate a tangent series to the point of convergence by dividing two other infinite series through calculating the compositional inverse of their Cauchy products (the convolution between terms) in roughly one-third of the time it took to add two regular Clojure vectors. This is the most expensive operation in the library and surely could be accelerated further by both sacrificing the integrity of the divisor and dividend and, more importantly, giving type hints so as to avoid conversion of ratios to BigIntegers (the latter applying to nearly all series produced by the library).

Infinite series are somewhat paradoxical benchmarks because, although they grow linearly with number, the rate is exponential with regards to storage due to the expansion of each term. However, because these particular series are convergent what would be expected to be exponential turns out to be constant rather quickly. For example, cosine and most similar recursive series complete in only roughly three times the overhead. In that sense, it's quite a different metric for lazy evaluation than the Sieve of Eratosthenes, where computation takes longer the further it progresses. I would be interested to see comparisons between languages for both types of lazy numeric computation, particularly with the Haskell versions.

I'd like to see more comparison of benchmarks for common numeric applications between Clojure and other languages in general, especially outside of the JVM. It seems like we're at a great time as far as library and community support to position it as a general option for a dynamic language to use for HPC, among others.

Lastly, and I know brevity isn't my strong suit, I seem to always leave even the tiniest project with a new weird idea and wanted to float it here. I had the idea for a lazy version of both BigInteger and BigDecimal types that would allow essentially limitless precision at no storage cost, with of course the oddity that one would be streaming only a portion of the large number. Can anyone think of use cases for this? Applications involving lossless compression, such as arithmetic coding come to mind, but other than having implemented versions as exercises that's far from my area of expertise. It seems almost trivial to implement, so I'll likely end up doing it regardless and come back to share :)

Thanks,
Sophia
